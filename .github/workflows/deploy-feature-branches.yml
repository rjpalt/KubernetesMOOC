name: Feature Branch Deployment

on:
  workflow_run:
    workflows: ["CI Pipeline - Microservices"]
    types: [completed]

permissions:
  id-token: write    # Required for Azure authentication
  contents: read     # Required for checkout
  pull-requests: write # Required for commenting on PRs

env:
  ACR_LOGIN_SERVER: ${{ vars.ACR_LOGIN_SERVER }}
  AKS_CLUSTER_NAME: ${{ vars.AKS_CLUSTER_NAME }}
  AKS_RESOURCE_GROUP: ${{ vars.AKS_RESOURCE_GROUP }}
  MANAGED_IDENTITY: ${{ vars.MANAGED_IDENTITY }}

jobs:
  deploy-feature-environment:
    name: Deploy Feature Environment
    runs-on: ubuntu-latest
    # Only run when CI succeeded AND the source (head) branch is not main
    if: >
      github.event.workflow_run.conclusion == 'success' &&
      github.event.workflow_run.head_branch != 'main'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        # CRITICAL: Checkout the exact same commit that triggered the CI pipeline
        # This ensures we deploy manifests that match the tested images
        ref: ${{ github.event.workflow_run.head_sha }}
    
    - name: Set environment variables
      run: |
        # Use the same branch and commit SHA as the triggering CI pipeline
        # This ensures image tags match: {branch}-{sha}
        BRANCH_NAME="${{ github.event.workflow_run.head_branch }}"
        COMMIT_SHA="${{ github.event.workflow_run.head_sha }}"
        NAMESPACE="feature-${BRANCH_NAME}"
        
        echo "BRANCH_NAME=${BRANCH_NAME}" >> $GITHUB_ENV
        echo "COMMIT_SHA=${COMMIT_SHA}" >> $GITHUB_ENV
        echo "NAMESPACE=${NAMESPACE}" >> $GITHUB_ENV
        echo "IMAGE_TAG_SUFFIX=${BRANCH_NAME}-${COMMIT_SHA}" >> $GITHUB_ENV
        
        echo "## Feature Environment Deployment ðŸš€" >> $GITHUB_STEP_SUMMARY
        echo "**Branch:** \`${BRANCH_NAME}\`" >> $GITHUB_STEP_SUMMARY
        echo "**Commit:** \`${COMMIT_SHA}\`" >> $GITHUB_STEP_SUMMARY
        echo "**Namespace:** \`${NAMESPACE}\`" >> $GITHUB_STEP_SUMMARY
    
    - name: Azure Login via Workload Identity
      uses: azure/login@v2
      with:
        client-id: ${{ secrets.AZURE_CLIENT_ID }}
        tenant-id: ${{ secrets.AZURE_TENANT_ID }}
        subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
    
    - name: Connect to AKS cluster
      run: |
        az aks get-credentials \
          --resource-group ${{ env.AKS_RESOURCE_GROUP }} \
          --name ${{ env.AKS_CLUSTER_NAME }}
    
    - name: Verify images exist in registry
      run: |
        echo "Verifying that tested images exist in registry..."
        IMAGE_SUFFIX="${{ env.IMAGE_TAG_SUFFIX }}"
        
        # Check if all required images exist and fail if any are missing
        echo "Checking backend image..."
        BACKEND_TAGS=$(az acr repository show-tags \
          --name $(echo "${{ env.ACR_LOGIN_SERVER }}" | cut -d'.' -f1) \
          --repository todo-app-be \
          --query "[?contains(@, '${IMAGE_SUFFIX}')]" \
          --output tsv)
        
        if [ -z "$BACKEND_TAGS" ]; then
          echo "âŒ Backend image with tag containing '${IMAGE_SUFFIX}' not found!"
          exit 1
        fi
        
        echo "Checking frontend image..."
        FRONTEND_TAGS=$(az acr repository show-tags \
          --name $(echo "${{ env.ACR_LOGIN_SERVER }}" | cut -d'.' -f1) \
          --repository todo-app-fe \
          --query "[?contains(@, '${IMAGE_SUFFIX}')]" \
          --output tsv)
        
        if [ -z "$FRONTEND_TAGS" ]; then
          echo "âŒ Frontend image with tag containing '${IMAGE_SUFFIX}' not found!"
          exit 1
        fi
        
        echo "Checking cron image..."
        CRON_TAGS=$(az acr repository show-tags \
          --name $(echo "${{ env.ACR_LOGIN_SERVER }}" | cut -d'.' -f1) \
          --repository todo-app-cron \
          --query "[?contains(@, '${IMAGE_SUFFIX}')]" \
          --output tsv)
        
        if [ -z "$CRON_TAGS" ]; then
          echo "âŒ Cron image with tag containing '${IMAGE_SUFFIX}' not found!"
          exit 1
        fi
        
        echo "âœ… All tested images found in registry:"
        echo "  - Backend: $BACKEND_TAGS"
        echo "  - Frontend: $FRONTEND_TAGS"
        echo "  - Cron: $CRON_TAGS"
    
    - name: Set kubectl context to feature namespace
      run: |

        # Set context to use the feature namespace (should already exist from provisioning workflow)
        kubectl config set-context --current --namespace=${{ env.NAMESPACE }}
        echo "âœ… kubectl context set to namespace: ${{ env.NAMESPACE }}"
    
    - name: Deploy feature environment
      run: |
        cd course_project/manifests/overlays/feature
        
        # Configure namespace for this feature branch
        kustomize edit set namespace ${{ env.NAMESPACE }}
        
        # Set the tested image tags
        kustomize edit set image \
          ${{ env.ACR_LOGIN_SERVER }}/todo-app-be=${{ env.ACR_LOGIN_SERVER }}/todo-app-be:${{ env.IMAGE_TAG_SUFFIX }} \
          ${{ env.ACR_LOGIN_SERVER }}/todo-app-fe=${{ env.ACR_LOGIN_SERVER }}/todo-app-fe:${{ env.IMAGE_TAG_SUFFIX }} \
          ${{ env.ACR_LOGIN_SERVER }}/todo-app-cron=${{ env.ACR_LOGIN_SERVER }}/todo-app-cron:${{ env.IMAGE_TAG_SUFFIX }}
        
        # Replace BRANCH_NAME placeholders in HTTPRoute patches with actual branch name
        # Handle both hostname (BRANCH_NAME) and database name (BRANCH_NAME_DB) placeholders
        DB_NAME=$(echo "${{ env.BRANCH_NAME }}" | tr '-' '_')
        sed -i "s/BRANCH_NAME_DB/${DB_NAME}/g" kustomization.yaml
        sed -i "s/BRANCH_NAME/${{ env.BRANCH_NAME }}/g" kustomization.yaml
        
        # Apply the manifests with feature-specific hostname routing configured
        kustomize build . | kubectl apply -f -
        
        echo "âœ… Feature environment deployed with AGC hostname: ${{ env.BRANCH_NAME }}.23.98.101.23.nip.io"

    
    - name: Wait for deployments
      run: |
        echo "Waiting for deployments to be ready..."
        
        # Note: No PostgreSQL StatefulSet to wait for - using Azure Database for PostgreSQL
        echo "Using Azure Database for PostgreSQL (no StatefulSet to wait for)"
        
        # Use deployment rollout status instead of individual pod waits
        # This avoids race conditions with terminating pods during rolling updates
        kubectl rollout status deployment/todo-app-be --timeout=300s
        echo "âœ… Backend deployment ready"
        
        kubectl rollout status deployment/todo-app-fe --timeout=300s
        echo "âœ… Frontend deployment ready"
    
    - name: Verify feature environment health
      run: |
        echo "Verifying feature environment health..."
        
        # Get service information
        kubectl get services -o wide
        
        # Find a ready backend pod using field selector (avoids terminating pods)
        echo "Finding ready backend pod..."
        BACKEND_POD=$(kubectl get pods -n ${{ env.NAMESPACE }} -l app=todo-app-be --field-selector=status.phase=Running -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo "")
        
        if [ -z "$BACKEND_POD" ]; then
          echo "No running backend pods found, waiting briefly..."
          sleep 10
          BACKEND_POD=$(kubectl get pods -n ${{ env.NAMESPACE }} -l app=todo-app-be --field-selector=status.phase=Running -o jsonpath='{.items[0].metadata.name}')
        fi
        
        echo "Found backend pod: $BACKEND_POD"
        
        # Debug: Check what's actually running in the backend pod
        echo "Debugging backend pod configuration..."
        kubectl describe pod $BACKEND_POD -n ${{ env.NAMESPACE }}
        echo "Checking backend pod logs..."
        kubectl logs $BACKEND_POD -n ${{ env.NAMESPACE }} --tail=20
        echo "Checking if backend is listening on expected ports..."
        kubectl exec $BACKEND_POD -n ${{ env.NAMESPACE }} -- netstat -tlnp 2>/dev/null || kubectl exec $BACKEND_POD -n ${{ env.NAMESPACE }} -- ss -tlnp 2>/dev/null || echo "netstat/ss not available"
        
        # Test backend health with port-forward to the correct port
        echo "Testing backend health..."
        kubectl port-forward pod/$BACKEND_POD 8001:2506 &
        PORT_FORWARD_PID=$!
        sleep 5
        
        if curl -f --max-time 10 http://localhost:8001/be-health; then
          echo "âœ… Backend health check passed"
        else
          echo "âŒ Backend health check failed, trying alternative ports..."
          kill $PORT_FORWARD_PID 2>/dev/null || true
          
          # Try port 8001 (common local development port)
          echo "Trying port-forward to 8001:8001..."
          kubectl port-forward pod/$BACKEND_POD 8002:8001 &
          PORT_FORWARD_PID=$!
          sleep 5
          
          if curl -f --max-time 10 http://localhost:8002/be-health; then
            echo "âœ… Backend health check passed on port 8001"
          else
            echo "âŒ Backend health check failed on both ports"
            kill $PORT_FORWARD_PID 2>/dev/null || true
            exit 1
          fi
        fi
        
        kill $PORT_FORWARD_PID 2>/dev/null || true
        
        # Find a ready frontend pod using field selector
        echo "Finding ready frontend pod..."
        FRONTEND_POD=$(kubectl get pods -n ${{ env.NAMESPACE }} -l app=todo-app-fe --field-selector=status.phase=Running -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo "")

        
        if [ -z "$FRONTEND_POD" ]; then
          echo "No running frontend pods found, waiting briefly..."
          sleep 10
          FRONTEND_POD=$(kubectl get pods -n ${{ env.NAMESPACE }} -l app=todo-app-fe --field-selector=status.phase=Running -o jsonpath='{.items[0].metadata.name}')

        fi
        
        echo "Found frontend pod: $FRONTEND_POD"
        
        # Debug: Check what's actually running in the frontend pod
        echo "Debugging frontend pod configuration..."
        kubectl describe pod $FRONTEND_POD -n ${{ env.NAMESPACE }}
        echo "Checking frontend pod logs..."
        kubectl logs $FRONTEND_POD -n ${{ env.NAMESPACE }} --tail=20
        
        # Test frontend health with port-forward
        echo "Testing frontend health..."
        kubectl port-forward pod/$FRONTEND_POD 8000:2507 &
        PORT_FORWARD_PID=$!
        sleep 5
        
        if curl -f --max-time 10 http://localhost:8000/health; then
          echo "âœ… Frontend health check passed"
        else
          echo "âŒ Frontend health check failed, trying alternative ports..."
          kill $PORT_FORWARD_PID 2>/dev/null || true
          
          # Try port 8000 (common local development port)
          echo "Trying port-forward to 8000:8000..."
          kubectl port-forward pod/$FRONTEND_POD 8003:8000 &
          PORT_FORWARD_PID=$!
          sleep 5
          
          if curl -f --max-time 10 http://localhost:8003/health; then
            echo "âœ… Frontend health check passed on port 8000"
          else
            echo "âŒ Frontend health check failed on both ports"
            kill $PORT_FORWARD_PID 2>/dev/null || true
            exit 1
          fi
        fi
        
        kill $PORT_FORWARD_PID 2>/dev/null || true
        echo "âœ… All health checks passed"
    
    - name: Cleanup on failure
      if: failure()
      run: |
        echo "Cleaning up resources due to deployment failure..."
        echo "Note: Infrastructure cleanup will be handled by the cleanup workflow when branch is deleted"
        echo "âœ… Application cleanup completed"
    
    - name: Comment on PR with feature environment details
      if: always() && !failure()
      run: |
        # Get the PR number from the workflow run
        PR_NUMBER=$(gh pr list --head "${{ env.BRANCH_NAME }}" --json number --jq '.[0].number')
        
        if [ -n "$PR_NUMBER" ] && [ "$PR_NUMBER" != "null" ]; then
          echo "Found PR #$PR_NUMBER for branch ${{ env.BRANCH_NAME }}"
          
          # Create comment body
          cat > pr_comment.md << EOF
        ## ðŸš€ Feature Environment Deployed Successfully
        
        Your feature branch \`${{ env.BRANCH_NAME }}\` has been deployed to its own isolated environment!
        
        ### ðŸŒ Access Information
        - **Feature URL**: http://${{ env.BRANCH_NAME }}.23.98.101.23.nip.io/
        - **Backend Health**: http://${{ env.BRANCH_NAME }}.23.98.101.23.nip.io/be-health
        - **API Documentation**: http://${{ env.BRANCH_NAME }}.23.98.101.23.nip.io/docs/
        - **Namespace**: \`${{ env.NAMESPACE }}\`
        
        ### ðŸ“Š Deployment Details
        - **Images**: \`${{ env.IMAGE_TAG_SUFFIX }}\`
        - **Database**: Azure PostgreSQL (provisioned when branch was created)
        - **Infrastructure**: Pre-provisioned by infrastructure workflow
        - **Gateway**: Azure Application Gateway for Containers (AGC)
        - **DNS**: nip.io (automatic DNS resolution)
        
        ### ðŸ§ª Testing
        You can now test your changes in this isolated environment. The environment includes:
        - Full backend API with isolated PostgreSQL database
        - Frontend application
        - Background cron jobs
        - All microservices running with your latest changes
        
        The environment will be automatically cleaned up when this branch is deleted.
        
        ---
        *Deployed by GitHub Actions at $(date -u)*
        EOF
          
          # Post comment to PR
          gh pr comment "$PR_NUMBER" --body-file pr_comment.md
          echo "âœ… Posted feature environment details to PR #$PR_NUMBER"
        else
          echo "âš ï¸ Could not find PR for branch ${{ env.BRANCH_NAME }}, skipping comment"
        fi
      env:
        GH_TOKEN: ${{ github.token }}
    
    - name: Generate deployment summary
      run: |
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "## Deployment Results ðŸ“Š" >> $GITHUB_STEP_SUMMARY
        echo "| Component | Status | Image |" >> $GITHUB_STEP_SUMMARY
        echo "|-----------|--------|-------|" >> $GITHUB_STEP_SUMMARY
        echo "| Backend | âœ… Deployed | \`kubemooc.azurecr.io/todo-app-be:${{ env.IMAGE_TAG_SUFFIX }}\` |" >> $GITHUB_STEP_SUMMARY
        echo "| Frontend | âœ… Deployed | \`kubemooc.azurecr.io/todo-app-fe:${{ env.IMAGE_TAG_SUFFIX }}\` |" >> $GITHUB_STEP_SUMMARY
        echo "| Cron | âœ… Deployed | \`kubemooc.azurecr.io/todo-app-cron:${{ env.IMAGE_TAG_SUFFIX }}\` |" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "## Access Information ðŸŒ" >> $GITHUB_STEP_SUMMARY
        echo "**Namespace:** \`${{ env.NAMESPACE }}\`" >> $GITHUB_STEP_SUMMARY

        echo "**Feature URL:** http://${{ env.BRANCH_NAME }}.23.98.101.23.nip.io/" >> $GITHUB_STEP_SUMMARY
        echo "**Backend Health:** http://${{ env.BRANCH_NAME }}.23.98.101.23.nip.io/be-health" >> $GITHUB_STEP_SUMMARY
        echo "**API Docs:** http://${{ env.BRANCH_NAME }}.23.98.101.23.nip.io/docs/" >> $GITHUB_STEP_SUMMARY

        echo "**Health Checks:** âœ… All services responding" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "## Next Steps ðŸ“" >> $GITHUB_STEP_SUMMARY
        echo "- Test your feature in the isolated environment" >> $GITHUB_STEP_SUMMARY
        echo "- Run E2E tests against this environment" >> $GITHUB_STEP_SUMMARY
        echo "- Environment will be cleaned up when branch is deleted" >> $GITHUB_STEP_SUMMARY
